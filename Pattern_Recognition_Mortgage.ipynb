{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tslearn import clustering\n",
    "import matplotlib.pylab as plt\n",
    "from tslearn.clustering import KShape\n",
    "import matplotlib as mpl\n",
    "from sklearn import metrics\n",
    "from tslearn.metrics import soft_dtw\n",
    "from tslearn.metrics import dtw\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case we will analyze mortgage data. Input file has monthly state level Unemployment rate and DQ Pct from 2007 to 2020\n",
    "# -- we will group 6-month state level Unemployment rate into different clusters\n",
    "# -- and then analyze the 60-day Delinquency rate of change within a specific cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input_data(path1, path2):\n",
    "    xl = pd.ExcelFile(path1)\n",
    "    dfMacro = xl.parse(xl.sheet_names[0])\n",
    "\n",
    "    xl = pd.ExcelFile(path2)\n",
    "    dfDQ = xl.parse(xl.sheet_names[0])\n",
    "    dfDQ[\"Date\"] = pd.to_datetime(dfDQ['Date'],format=\"%m/%d/%Y\")\n",
    "    return dfMacro,dfDQ\n",
    "\n",
    "def generate_training_data(df, length, step, start_index, end_index, col):\n",
    "    # Store the start index of each time series in the training data\n",
    "    X_index = []\n",
    "    bStart = True\n",
    "    index = start_index\n",
    "    fullIndex = start_index\n",
    "    states = df.columns.to_list()\n",
    "    states.remove(\"Date\")\n",
    "    \n",
    "    while index < end_index:\n",
    "        index += step\n",
    "        insideStep = 0\n",
    "        for state in states:\n",
    "            X_index.append([index,state])\n",
    "            series = df[state].iloc[index:index+length].values.reshape(1,-1)\n",
    "            series = preprocessing.normalize(series)\n",
    "            if bStart:\n",
    "                X_train = series\n",
    "                bStart = False\n",
    "            else:\n",
    "                # concat new time series to training data\n",
    "                X_train = np.vstack((X_train,series))\n",
    "        index += step\n",
    "    X_index = np.asarray(X_index)\n",
    "    return (X_train, X_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input UE and DQ data.\n",
    "# Take 6-month UE as training data, so each time series of length 6\n",
    "# We will normarlize each observation in L2 form, which means if each element were squared and summed, the total would equal 1\n",
    "dataMacro,dataDQ = read_input_data(\"StateUE_noDakotas.xls\",\"MortgageDelinquency.xlsx\")\n",
    "targetField = \"DQ60Pct\"\n",
    "n_cluster = 10\n",
    "length = 6\n",
    "step = 5\n",
    "start_index = 0\n",
    "dqMonths = 3\n",
    "end_index = len(dataMacro) - 100\n",
    "X_train, X_index = generate_training_data(dataMacro, length, step, start_index, end_index, 'Close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Sklearn TimeSeriesKMeans model with metric dtw\n",
    "# Store predicted labels on labels_train\n",
    "metric = 'dtw'\n",
    "km = TimeSeriesKMeans(n_clusters=n_cluster,metric=metric)\n",
    "labels_train = km.fit_predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a scoring function to measure the cluster performance\n",
    "# introduce Silhouette coefficient \n",
    "# -- a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation)\n",
    "\n",
    "# compute dtw similarity between two time series\n",
    "def calculate_dist(X, Y):\n",
    "    return dtw(X,Y)\n",
    "\n",
    "# compute average dtw distance between a time series and a whole cluster\n",
    "def calculate_mean_dist(X, arr, mode):\n",
    "    nsample = arr.shape[0]\n",
    "    total = 0\n",
    "    if mode == 0: # within cluster\n",
    "        for series in arr:\n",
    "            if np.array_equal(X, series):\n",
    "                continue\n",
    "            dist = calculate_dist(X, series)\n",
    "            total += dist\n",
    "        return (total / (nsample - 1))\n",
    "    else: # inter-cluster\n",
    "        for series in arr:\n",
    "            dist = calculate_dist(X, series)\n",
    "            total += dist\n",
    "        return (total / nsample)\n",
    "    \n",
    "# compute the average silhouette score of a specific cluster\n",
    "'''\n",
    "for a given cluster, loop through each time series in the cluster, calculate its silhouette coefficient\n",
    "and then return the average as silhouette score for the entire cluster\n",
    "coefficient calculated based on within-cluster distance and min inter-cluster distance\n",
    "'''\n",
    "def calculate_cluster_score(i_cluster, df, labels, n_cluster):\n",
    "    X_cluster_in = df[labels == i_cluster]\n",
    "    cluster_score = 0\n",
    "    count = 0\n",
    "    for obs in X_cluster_in:\n",
    "        d_in = calculate_mean_dist(obs, X_cluster_in, 0)\n",
    "        count += 1\n",
    "        bFirst = True\n",
    "        for x_cluster in range(n_cluster):\n",
    "            if x_cluster == i_cluster:\n",
    "                continue\n",
    "            X_cluster_out = df[labels == x_cluster]\n",
    "            d_out = calculate_mean_dist(obs, X_cluster_out, 1)\n",
    "            if bFirst:\n",
    "                d_out_min = d_out\n",
    "                bFirst = False\n",
    "            else:\n",
    "                if d_out < d_out_min:\n",
    "                    d_out_min = d_out\n",
    "        d_score = (d_out_min - d_in) / (max(d_out_min, d_in))\n",
    "        cluster_score += d_score\n",
    "    return (i_cluster, (cluster_score / count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the score function\n",
    "cluster_quality = []\n",
    "for i in range(n_cluster):\n",
    "    cluster_quality.append(calculate_cluster_score(i, X_train, labels_train, n_cluster))\n",
    "cluster_quality = sorted(cluster_quality,reverse=True, key=lambda x: x[1])\n",
    "cluster_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we want to analyze the rate of change for Delinquency data\n",
    "# for each time series in the cluster, we select end month T and then calculate the DQ60 rate of change at T+1 (1 month later)\n",
    "def get_cluster_dq(dfMacro, dfDQ, index_arr, dqMonths, step):\n",
    "    dq_arr = []\n",
    "    for stateInd in index_arr:\n",
    "        macroIndex = int(stateInd[0])\n",
    "        try:\n",
    "            r1 = dfDQ.loc[(dfDQ[\"State\"] == stateInd[1]) & (dfDQ[\"Date\"] == dfMacro.iloc[macroIndex + step][\"Date\"]),targetField].values[0]\n",
    "            r2 = dfDQ.loc[(dfDQ[\"State\"] == stateInd[1]) & (dfDQ[\"Date\"] == dfMacro.iloc[macroIndex + step + 1][\"Date\"]),targetField].values[0]\n",
    "            r = (r2 / r1 - 1) * 100\n",
    "        except:\n",
    "            print(\"data not found for \", stateInd[1], dfMacro.iloc[macroIndex + step][\"Date\"])\n",
    "            r = 0\n",
    "        dq_arr.append(r)\n",
    "    dq_arr = np.asarray(dq_arr)\n",
    "    return dq_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Unemployment rate trend and DQ rate of change graphs for the three best clusters\n",
    "fig, ax = plt.subplots(2, 3, figsize=(20,10))\n",
    "fig.suptitle('Unemployment and DQ rate of change plot')\n",
    "index = 0\n",
    "for clusterNum in range(3):\n",
    "    cluster = cluster_quality[clusterNum][0]\n",
    "    X_cluster = X_train[labels_train == cluster]\n",
    "    X_cluster_index = X_index[labels_train == cluster]\n",
    "    dq_arr = get_cluster_dq(dataMacro, dataDQ, X_cluster_index, dqMonths, step)\n",
    "    ax[0, index].plot(range(1,length+1),np.median(X_cluster, axis=0))\n",
    "    ax[0, index].set_title('Unemployment rate')\n",
    "    sort_return = np.sort(dq_arr)\n",
    "    y = np.arange(1, len(sort_return)+1) / len(sort_return)\n",
    "    med_val = np.median(sort_return)\n",
    "    median = np.array([med_val for i in range(len(y))])\n",
    "    ax[1, index].plot(sort_return, y, marker='.', linestyle='none')\n",
    "    ax[1, index].plot(median, y, label='median')\n",
    "    ax[1, index].set_title('ECDF plot for DQ rate of change')\n",
    "    ax[1, index].set_xticks(np.arange(np.floor(np.min(sort_return)),np.ceil(np.max(sort_return)) + 1,step=3))\n",
    "    ax[1, index].set_yticks(np.arange(0,1.1,step=0.1))\n",
    "    ax[1, index].set_xlabel(targetField)\n",
    "    ax[1, index].set_ylabel('ECDF')\n",
    "    ax[1, index].legend()\n",
    "    index += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
